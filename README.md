# ğŸŒ Climate Intelligence Platform for Extreme Weather Prediction

[![Python](https://img.shields.io/badge/Python-3.10+-blue.svg)](https://python.org)
[![Apache Kafka](https://img.shields.io/badge/Apache%20Kafka-3.6-red.svg)](https://kafka.apache.org)
[![Apache Spark](https://img.shields.io/badge/Apache%20Spark-3.5-orange.svg)](https://spark.apache.org)
[![Docker](https://img.shields.io/badge/Docker-Containerized-2496ED.svg)](https://docker.com)
[![GCP](https://img.shields.io/badge/GCP-Deployed-4285F4.svg)](https://cloud.google.com)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)

> A production-grade, end-to-end climate intelligence platform that ingests real-time weather data, processes it through a medallion data lake, predicts extreme weather events using ML/Deep Learning, and provides Gen AI-powered natural language insights â€” all deployed on Google Cloud Platform with Docker & Kubernetes.

---

## ğŸ“‹ Table of Contents

- [Overview](#overview)
- [Architecture](#architecture)
- [Tech Stack](#tech-stack)
- [Features](#features)
- [Project Structure](#project-structure)
- [Getting Started](#getting-started)
- [Pipeline Deep Dive](#pipeline-deep-dive)
- [ML Models](#ml-models)
- [Gen AI Integration](#gen-ai-integration)
- [Deployment](#deployment)
- [Monitoring](#monitoring)
- [API Documentation](#api-documentation)
- [Contributing](#contributing)
- [License](#license)

---

## ğŸ¯ Overview

### The Problem
Extreme weather events (heatwaves, floods, hurricanes, storms) are increasing in frequency and severity. Early prediction can save lives, reduce economic damage, and help communities prepare. Current systems often lack real-time processing, explainability, and accessible interfaces for non-technical users.

### The Solution
This platform combines **real-time data engineering**, **machine learning**, and **generative AI** to:
- Ingest live weather streams from multiple sources via Apache Kafka
- Process terabytes of climate data using Apache Spark (batch + streaming)
- Predict extreme weather events 24-72 hours in advance using ensemble ML models
- Generate human-readable weather intelligence reports using LLMs
- Allow users to query climate data in natural language via a RAG-powered chatbot

### Who Is This For?
- Emergency response agencies needing early warning systems
- Insurance companies assessing climate risk
- Researchers studying extreme weather patterns
- City planners building climate-resilient infrastructure

---

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                          DATA SOURCES                                     â”‚
â”‚   NOAA API  â”‚  OpenWeatherMap API  â”‚  NASA Satellite  â”‚  Weather CSVs    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 INGESTION LAYER (Real-Time Streaming)                     â”‚
â”‚                                                                           â”‚
â”‚   Python Kafka Producers â”€â”€â†’ Apache Kafka (3-Broker Cluster)             â”‚
â”‚                                â”œâ”€â”€ Topic: raw-weather-data               â”‚
â”‚                                â”œâ”€â”€ Topic: weather-alerts                 â”‚
â”‚                                â””â”€â”€ Topic: satellite-metadata             â”‚
â”‚                                                                           â”‚
â”‚   Confluent Schema Registry (Avro) â”€â”€ Data contracts & validation        â”‚
â”‚   Kafka Connect â”€â”€ GCS Sink Connector (raw archival)                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              PROCESSING LAYER (Batch + Stream)                            â”‚
â”‚                                                                           â”‚
â”‚   Spark Structured Streaming                                             â”‚
â”‚     â””â”€â”€ Kafka â†’ Bronze Layer (real-time, sub-second latency)             â”‚
â”‚                                                                           â”‚
â”‚   Spark Batch Jobs (Medallion Architecture)                              â”‚
â”‚     â”œâ”€â”€ Bronze â†’ Silver : Deduplication, null handling, type casting     â”‚
â”‚     â”œâ”€â”€ Silver â†’ Gold   : Aggregations, feature engineering, indexing    â”‚
â”‚     â””â”€â”€ Gold  â†’ BigQuery: Analytics-ready dimensional tables             â”‚
â”‚                                                                           â”‚
â”‚   Great Expectations â”€â”€ Data quality validation at every layer           â”‚
â”‚   Apache Airflow â”€â”€ DAG orchestration for all pipelines                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                       STORAGE LAYER                                       â”‚
â”‚                                                                           â”‚
â”‚   Google Cloud Storage (Data Lake)                                       â”‚
â”‚     â”œâ”€â”€ gs://climate-bronze/  â†’ Raw Parquet (as received)                â”‚
â”‚     â”œâ”€â”€ gs://climate-silver/  â†’ Cleaned & validated                      â”‚
â”‚     â””â”€â”€ gs://climate-gold/    â†’ Feature-engineered & aggregated          â”‚
â”‚                                                                           â”‚
â”‚   Google BigQuery (Data Warehouse)                                       â”‚
â”‚     â”œâ”€â”€ Fact: fact_weather_readings, fact_predictions                    â”‚
â”‚     â”œâ”€â”€ Dim:  dim_location, dim_time, dim_weather_type                  â”‚
â”‚     â””â”€â”€ ML:   feature_store_weather                                      â”‚
â”‚                                                                           â”‚
â”‚   PostgreSQL (Application metadata & user state)                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        ML / AI LAYER                                      â”‚
â”‚                                                                           â”‚
â”‚   Traditional ML                                                         â”‚
â”‚     â”œâ”€â”€ XGBoost â”€â”€ Extreme weather classification                        â”‚
â”‚     â”œâ”€â”€ SHAP â”€â”€ Feature importance & explainability                      â”‚
â”‚     â””â”€â”€ Optuna â”€â”€ Hyperparameter optimization                            â”‚
â”‚                                                                           â”‚
â”‚   Deep Learning (PyTorch)                                                â”‚
â”‚     â”œâ”€â”€ LSTM â”€â”€ Time-series forecasting (temperature, pressure)          â”‚
â”‚     â”œâ”€â”€ GRU â”€â”€ Comparison variant                                        â”‚
â”‚     â””â”€â”€ Uncertainty quantification (confidence intervals)                â”‚
â”‚                                                                           â”‚
â”‚   Ensemble                                                               â”‚
â”‚     â””â”€â”€ Weighted voting: XGBoost + LSTM combined predictions             â”‚
â”‚                                                                           â”‚
â”‚   MLflow â”€â”€ Experiment tracking, model versioning, registry              â”‚
â”‚   Vertex AI â”€â”€ Production model serving (auto-scaling endpoints)         â”‚
â”‚   Evidently AI â”€â”€ Data drift & model performance monitoring              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      GEN AI LAYER                                         â”‚
â”‚                                                                           â”‚
â”‚   Natural Language Report Generator                                      â”‚
â”‚     â””â”€â”€ Raw predictions â†’ LLM â†’ Human-readable weather intelligence      â”‚
â”‚         Example: "Severe heatwave expected in Phoenix, AZ. Temperatures  â”‚
â”‚         will exceed 115Â°F over the next 72 hours. Risk level: HIGH."     â”‚
â”‚                                                                           â”‚
â”‚   Anomaly Explanation Engine                                             â”‚
â”‚     â””â”€â”€ Model detects anomaly â†’ LLM explains WHY in plain English        â”‚
â”‚                                                                           â”‚
â”‚   RAG-Powered Climate Chatbot                                            â”‚
â”‚     â””â”€â”€ User: "What caused the 2024 Texas floods?"                       â”‚
â”‚         â†’ Retrieves from ChromaDB â†’ LLM generates grounded answer        â”‚
â”‚                                                                           â”‚
â”‚   Text-to-SQL Query Engine                                               â”‚
â”‚     â””â”€â”€ User: "Show heatwave predictions for California next week"       â”‚
â”‚         â†’ Converts to SQL â†’ Queries BigQuery â†’ Returns natural response  â”‚
â”‚                                                                           â”‚
â”‚   ChromaDB (Vector Store) + Sentence-Transformers (Embeddings)           â”‚
â”‚   LangChain (Orchestration) + Gemini / Claude API (LLM)                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   SERVING & VISUALIZATION                                 â”‚
â”‚                                                                           â”‚
â”‚   FastAPI Backend                                                        â”‚
â”‚     â”œâ”€â”€ POST /predict       â†’ Weather predictions                        â”‚
â”‚     â”œâ”€â”€ POST /chat          â†’ Gen AI chatbot                             â”‚
â”‚     â”œâ”€â”€ GET  /anomalies     â†’ Anomaly explanations                       â”‚
â”‚     â”œâ”€â”€ GET  /reports       â†’ Generated weather reports                  â”‚
â”‚     â””â”€â”€ GET  /health        â†’ Service health check                       â”‚
â”‚                                                                           â”‚
â”‚   Streamlit Dashboard                                                    â”‚
â”‚     â”œâ”€â”€ Real-time weather map with prediction overlays                   â”‚
â”‚     â”œâ”€â”€ Historical trend analysis & model accuracy tracker               â”‚
â”‚     â”œâ”€â”€ Interactive chatbot tab (natural language queries)               â”‚
â”‚     â””â”€â”€ Model performance & drift monitoring panel                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  DEPLOYMENT & INFRASTRUCTURE                              â”‚
â”‚                                                                           â”‚
â”‚   Docker â”€â”€ Every service containerized (12+ Dockerfiles)                â”‚
â”‚   Docker Compose â”€â”€ Local development (single command startup)           â”‚
â”‚   GKE (Kubernetes) â”€â”€ Production container orchestration                 â”‚
â”‚   Terraform â”€â”€ Infrastructure as Code (all GCP resources)                â”‚
â”‚   GitHub Actions â”€â”€ CI/CD (lint, test, build, deploy)                    â”‚
â”‚   Prometheus + Grafana â”€â”€ Infrastructure monitoring & alerting           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ› ï¸ Tech Stack

| Category | Technologies | Purpose |
|----------|-------------|---------|
| **Data Ingestion** | Apache Kafka (3-broker), Schema Registry (Avro), Kafka Connect | Real-time streaming ingestion with data contracts |
| **Stream Processing** | Spark Structured Streaming | Sub-second processing from Kafka to data lake |
| **Batch Processing** | Apache Spark (PySpark), Spark SQL | Medallion transformations (Bronze â†’ Silver â†’ Gold) |
| **Orchestration** | Apache Airflow | DAG-based pipeline scheduling & dependency management |
| **Data Quality** | Great Expectations | Automated validation checkpoints at every layer |
| **Data Lake** | Google Cloud Storage (GCS) | Partitioned Parquet files in medallion layers |
| **Data Warehouse** | Google BigQuery | Star schema dimensional model for analytics |
| **Traditional ML** | XGBoost, Scikit-learn, SHAP, Optuna | Classification, explainability, hyperparameter tuning |
| **Deep Learning** | PyTorch (LSTM, GRU) | Time-series forecasting with uncertainty quantification |
| **ML Ops** | MLflow, Evidently AI | Experiment tracking, model registry, drift detection |
| **Model Serving** | Vertex AI Endpoints | Auto-scaling production inference |
| **Gen AI** | Gemini/Claude API, LangChain, ChromaDB | RAG chatbot, report generation, text-to-SQL |
| **Backend API** | FastAPI | High-performance async REST API |
| **Frontend** | Streamlit | Interactive dashboard with real-time visualizations |
| **Containerization** | Docker, Docker Compose | Service isolation & reproducible environments |
| **Cloud Platform** | GCP (GKE, GCS, BigQuery, Vertex AI, Cloud Run) | Production deployment |
| **Infrastructure as Code** | Terraform | Automated GCP resource provisioning |
| **CI/CD** | GitHub Actions | Automated testing, building, and deployment |
| **Monitoring** | Prometheus, Grafana | Infrastructure metrics & alerting |

---

## âœ¨ Features

### Data Engineering
- **Real-time ingestion** from 4+ weather data sources via Kafka
- **Schema enforcement** with Avro and Schema Registry (backward compatible evolution)
- **Medallion data lake** (Bronze â†’ Silver â†’ Gold) on GCS
- **Star schema** dimensional model in BigQuery
- **Automated data quality** checks with Great Expectations at every layer
- **Airflow DAGs** orchestrating ingestion, processing, training, and monitoring

### Machine Learning
- **Extreme weather classification** (flood, heatwave, storm, hurricane) with XGBoost
- **Time-series forecasting** (temperature, pressure, wind) with LSTM & GRU
- **Ensemble model** combining traditional ML + deep learning
- **Explainability** via SHAP values (know WHY the model predicted an event)
- **Uncertainty quantification** with confidence intervals on every prediction
- **Automated hyperparameter tuning** with Optuna
- **Full experiment tracking** with MLflow (metrics, parameters, artifacts)
- **Model drift detection** with Evidently AI

### Generative AI
- **Natural language weather reports** generated from raw model predictions
- **Anomaly explanation engine** â€” when models detect something unusual, the LLM explains why
- **RAG-powered chatbot** â€” ask climate questions grounded in your actual data
- **Text-to-SQL** â€” query BigQuery using natural language ("Show me all storms in Texas last month")

### Deployment & Operations
- **12+ Dockerized microservices** with Docker Compose for local development
- **Kubernetes (GKE)** manifests for production orchestration
- **Terraform** for all GCP infrastructure as code
- **GitHub Actions CI/CD** with automated lint, test, build, and deploy
- **Prometheus + Grafana** monitoring with custom dashboards

---

## ğŸ“ Project Structure

```
climate-intelligence-platform/
â”‚
â”œâ”€â”€ README.md
â”œâ”€â”€ docker-compose.yml                    # Start everything locally with one command
â”œâ”€â”€ .env.example                          # Environment variable template
â”œâ”€â”€ requirements.txt                      # Python dependencies
â”œâ”€â”€ Makefile                              # Common commands (make build, make test, etc.)
â”‚
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ workflows/
â”‚       â”œâ”€â”€ ci.yml                        # Lint + test on every pull request
â”‚       â””â”€â”€ cd.yml                        # Build + deploy on merge to main
â”‚
â”œâ”€â”€ infrastructure/
â”‚   â”œâ”€â”€ terraform/
â”‚   â”‚   â”œâ”€â”€ main.tf                       # GCS, BigQuery, GKE, Vertex AI resources
â”‚   â”‚   â”œâ”€â”€ variables.tf                  # Configurable parameters
â”‚   â”‚   â””â”€â”€ outputs.tf                    # Resource IDs & endpoints
â”‚   â””â”€â”€ kubernetes/
â”‚       â”œâ”€â”€ kafka-deployment.yml
â”‚       â”œâ”€â”€ spark-deployment.yml
â”‚       â”œâ”€â”€ api-deployment.yml
â”‚       â”œâ”€â”€ dashboard-deployment.yml
â”‚       â””â”€â”€ monitoring-deployment.yml
â”‚
â”œâ”€â”€ ingestion/
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â”œâ”€â”€ kafka_producer_noaa.py            # NOAA API â†’ Kafka
â”‚   â”œâ”€â”€ kafka_producer_openweather.py     # OpenWeatherMap API â†’ Kafka
â”‚   â”œâ”€â”€ kafka_producer_nasa.py            # NASA satellite metadata â†’ Kafka
â”‚   â”œâ”€â”€ avro_schemas/
â”‚   â”‚   â”œâ”€â”€ weather_reading.avsc          # Schema for weather observations
â”‚   â”‚   â””â”€â”€ weather_alert.avsc           # Schema for severe weather alerts
â”‚   â””â”€â”€ kafka_connect/
â”‚       â””â”€â”€ gcs_sink_connector.json       # Auto-archive raw messages to GCS
â”‚
â”œâ”€â”€ processing/
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â”œâ”€â”€ spark_streaming_bronze.py         # Kafka â†’ Bronze (real-time)
â”‚   â”œâ”€â”€ spark_batch_silver.py             # Bronze â†’ Silver (cleaning)
â”‚   â”œâ”€â”€ spark_batch_gold.py               # Silver â†’ Gold (feature engineering)
â”‚   â”œâ”€â”€ spark_to_bigquery.py              # Gold â†’ BigQuery (warehouse load)
â”‚   â””â”€â”€ data_quality/
â”‚       â””â”€â”€ great_expectations/
â”‚           â”œâ”€â”€ expectations/
â”‚           â”‚   â”œâ”€â”€ bronze_suite.json     # Raw data expectations
â”‚           â”‚   â”œâ”€â”€ silver_suite.json     # Cleaned data expectations
â”‚           â”‚   â””â”€â”€ gold_suite.json       # Feature data expectations
â”‚           â””â”€â”€ checkpoints/
â”‚               â””â”€â”€ weather_checkpoint.yml
â”‚
â”œâ”€â”€ orchestration/
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ dags/
â”‚       â”œâ”€â”€ daily_ingestion_dag.py        # Scheduled data collection
â”‚       â”œâ”€â”€ batch_processing_dag.py       # Bronze â†’ Silver â†’ Gold â†’ BigQuery
â”‚       â”œâ”€â”€ model_training_dag.py         # Weekly retraining pipeline
â”‚       â””â”€â”€ model_monitoring_dag.py       # Daily drift detection
â”‚
â”œâ”€â”€ ml/
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â”œâ”€â”€ feature_engineering.py            # Create ML features from Gold data
â”‚   â”œâ”€â”€ train_xgboost.py                 # XGBoost classifier training
â”‚   â”œâ”€â”€ train_lstm.py                    # PyTorch LSTM training
â”‚   â”œâ”€â”€ train_gru.py                     # PyTorch GRU variant
â”‚   â”œâ”€â”€ ensemble_model.py               # Combine XGBoost + LSTM predictions
â”‚   â”œâ”€â”€ hyperparameter_tuning.py         # Optuna optimization
â”‚   â”œâ”€â”€ model_evaluation.py             # Metrics, SHAP, confusion matrix
â”‚   â””â”€â”€ mlflow_config.py                # MLflow tracking server config
â”‚
â”œâ”€â”€ genai/
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â”œâ”€â”€ rag_pipeline.py                  # ChromaDB retrieval + LLM generation
â”‚   â”œâ”€â”€ report_generator.py             # Predictions â†’ natural language reports
â”‚   â”œâ”€â”€ anomaly_explainer.py            # Anomaly â†’ LLM explanation
â”‚   â”œâ”€â”€ text_to_sql.py                  # Natural language â†’ BigQuery SQL
â”‚   â”œâ”€â”€ embeddings/
â”‚   â”‚   â””â”€â”€ embed_historical_reports.py  # Build vector store from climate docs
â”‚   â””â”€â”€ prompts/
â”‚       â”œâ”€â”€ report_prompt.txt            # Prompt template for reports
â”‚       â”œâ”€â”€ anomaly_prompt.txt           # Prompt template for anomaly explanation
â”‚       â””â”€â”€ sql_prompt.txt               # Prompt template for text-to-SQL
â”‚
â”œâ”€â”€ api/
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â”œâ”€â”€ main.py                          # FastAPI application entry point
â”‚   â”œâ”€â”€ routers/
â”‚   â”‚   â”œâ”€â”€ predictions.py               # /predict endpoints
â”‚   â”‚   â”œâ”€â”€ chat.py                      # /chat endpoints (Gen AI)
â”‚   â”‚   â”œâ”€â”€ anomalies.py                # /anomalies endpoints
â”‚   â”‚   â”œâ”€â”€ reports.py                  # /reports endpoints
â”‚   â”‚   â””â”€â”€ health.py                   # /health endpoint
â”‚   â””â”€â”€ schemas/
â”‚       â”œâ”€â”€ request_models.py            # Pydantic request schemas
â”‚       â””â”€â”€ response_models.py           # Pydantic response schemas
â”‚
â”œâ”€â”€ dashboard/
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â”œâ”€â”€ app.py                           # Streamlit main entry point
â”‚   â”œâ”€â”€ pages/
â”‚   â”‚   â”œâ”€â”€ 1_realtime_map.py           # Live weather map + predictions
â”‚   â”‚   â”œâ”€â”€ 2_historical_trends.py      # Trend analysis & charts
â”‚   â”‚   â”œâ”€â”€ 3_chatbot.py               # Gen AI chatbot interface
â”‚   â”‚   â””â”€â”€ 4_model_monitoring.py       # Drift & accuracy tracking
â”‚   â””â”€â”€ utils/
â”‚       â””â”€â”€ api_client.py               # Helper to call FastAPI backend
â”‚
â”œâ”€â”€ monitoring/
â”‚   â”œâ”€â”€ prometheus/
â”‚   â”‚   â””â”€â”€ prometheus.yml              # Scrape configs for all services
â”‚   â”œâ”€â”€ grafana/
â”‚   â”‚   â””â”€â”€ dashboards/
â”‚   â”‚       â”œâ”€â”€ pipeline_health.json    # Data pipeline metrics
â”‚   â”‚       â””â”€â”€ model_performance.json  # ML model metrics
â”‚   â””â”€â”€ evidently/
â”‚       â””â”€â”€ drift_detection.py          # Scheduled drift reports
â”‚
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_ingestion/
â”‚   â”‚   â””â”€â”€ test_kafka_producer.py
â”‚   â”œâ”€â”€ test_processing/
â”‚   â”‚   â””â”€â”€ test_spark_jobs.py
â”‚   â”œâ”€â”€ test_ml/
â”‚   â”‚   â””â”€â”€ test_model_training.py
â”‚   â”œâ”€â”€ test_genai/
â”‚   â”‚   â””â”€â”€ test_rag_pipeline.py
â”‚   â””â”€â”€ test_api/
â”‚       â””â”€â”€ test_endpoints.py
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ setup_local.sh                  # Install dependencies & start Docker
â”‚   â”œâ”€â”€ setup_gcp.sh                    # Create GCP project & enable APIs
â”‚   â”œâ”€â”€ create_kafka_topics.sh          # Initialize Kafka topics
â”‚   â””â”€â”€ load_historical_data.sh         # Backfill historical weather data
â”‚
â””â”€â”€ docs/
    â”œâ”€â”€ ARCHITECTURE.md                  # Detailed architecture decisions
    â”œâ”€â”€ DEPLOYMENT.md                    # Step-by-step cloud deployment guide
    â”œâ”€â”€ API.md                           # Full API reference
    â””â”€â”€ TROUBLESHOOTING.md              # Common issues & fixes
```

---

## ğŸš€ Getting Started

### Prerequisites

| Tool | Version | Installation |
|------|---------|-------------|
| Python | 3.10+ | [python.org](https://python.org) |
| Docker Desktop | Latest | [docker.com](https://docker.com/products/docker-desktop) |
| Git | Latest | [git-scm.com](https://git-scm.com) |
| Google Cloud SDK | Latest | [cloud.google.com/sdk](https://cloud.google.com/sdk/docs/install) |

### Quick Start (Local Development)

```bash
# 1. Clone the repository
git clone https://github.com/yourusername/climate-intelligence-platform.git
cd climate-intelligence-platform

# 2. Copy environment template and add your API keys
cp .env.example .env
# Edit .env with your NOAA_API_KEY, OPENWEATHER_API_KEY, GEMINI_API_KEY

# 3. Start all services (Kafka, Spark, Airflow, API, Dashboard, etc.)
docker-compose up -d

# 4. Verify services are running
docker-compose ps
```

### Access Points (Local)

| Service | URL | Description |
|---------|-----|-------------|
| **FastAPI** | http://localhost:8000 | REST API + Swagger docs at /docs |
| **Streamlit Dashboard** | http://localhost:8501 | Interactive visualization |
| **Airflow** | http://localhost:8080 | Pipeline orchestration UI |
| **Spark Master** | http://localhost:8090 | Spark job monitoring |
| **MLflow** | http://localhost:5000 | Experiment tracking UI |
| **Kafka UI** | http://localhost:9021 | Kafka topic monitoring |
| **Grafana** | http://localhost:3000 | Infrastructure dashboards |
| **Prometheus** | http://localhost:9090 | Metrics collection |

---

## ğŸ”„ Pipeline Deep Dive

### 1. Ingestion (Kafka Producers)

Weather data flows from multiple APIs into a 3-broker Kafka cluster. Each source has its own producer with Avro schema validation via Schema Registry. This ensures data contracts are enforced before any data enters the pipeline.

```
NOAA API â”€â”€â”€â”€â”€â”
               â”œâ”€â”€â†’ Kafka Broker Cluster â”€â”€â†’ 3 Topics (partitioned by region)
OpenWeather â”€â”€â”¤                               â”‚
               â”‚                               â”œâ”€â”€â†’ Spark Streaming (real-time)
NASA â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                               â””â”€â”€â†’ Kafka Connect â†’ GCS (archival)
```

### 2. Processing (Medallion Architecture)

| Layer | Purpose | Format | Example Transformation |
|-------|---------|--------|----------------------|
| **Bronze** | Raw data as-is | Parquet, partitioned by `ingestion_date` | Kafka JSON â†’ Parquet |
| **Silver** | Cleaned & validated | Parquet, partitioned by `date/region` | Remove nulls, fix types, deduplicate |
| **Gold** | Business-ready features | Parquet, partitioned by `date/region` | Rolling averages, heat index, anomaly flags |

### 3. Warehouse (BigQuery Star Schema)

```
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚  dim_time     â”‚
              â”‚  date_key     â”‚
              â”‚  hour, day    â”‚
              â”‚  month, year  â”‚
              â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ dim_location  â”œâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¤ fact_weather      â”‚
â”‚ location_key  â”‚     â”‚     â”‚ temperature       â”‚
â”‚ city, state   â”‚     â”‚     â”‚ humidity          â”‚
â”‚ lat, lon      â”‚     â”‚     â”‚ wind_speed        â”‚
â”‚ region        â”‚     â”‚     â”‚ pressure          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚     â”‚ precipitation     â”‚
                     â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”
              â”‚dim_weather   â”‚
              â”‚type          â”‚
              â”‚severity      â”‚
              â”‚category      â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ¤– ML Models

| Model | Task | Input | Output |
|-------|------|-------|--------|
| **XGBoost** | Classify extreme weather events | Gold layer features (30+ features) | Event type + probability |
| **LSTM** | Forecast temperature/pressure | 7-day sliding window time-series | 24-72 hour forecast + confidence interval |
| **GRU** | Benchmark comparison to LSTM | Same as LSTM | Same as LSTM |
| **Ensemble** | Final production prediction | XGBoost + LSTM outputs | Weighted combined prediction |

All experiments tracked in **MLflow** with metrics, parameters, and model artifacts. Best model auto-promoted to **Vertex AI** for serving.

---

## ğŸ§  Gen AI Integration

| Feature | How It Works | Example |
|---------|-------------|---------|
| **Weather Reports** | Model prediction â†’ LLM prompt â†’ Natural language report | *"A severe heatwave is expected in Phoenix, AZ over the next 72 hours with temperatures exceeding 115Â°F. Risk level: HIGH. Recommend increased water distribution and cooling center activation."* |
| **Anomaly Explainer** | Anomaly detected â†’ Historical context retrieved â†’ LLM explains | *"Unusual pressure drop detected in Gulf Coast region. Historically, this pattern preceded Category 3+ hurricanes 73% of the time."* |
| **RAG Chatbot** | Question â†’ Embed â†’ Retrieve from ChromaDB â†’ LLM answer | User: "What caused major flooding in Houston?" â†’ Grounded answer from historical data |
| **Text-to-SQL** | Natural language â†’ SQL query â†’ BigQuery â†’ Natural response | User: "How many storms hit Florida in 2024?" â†’ `SELECT COUNT(*)...` â†’ "There were 47 storm events recorded in Florida during 2024." |

---

## â˜ï¸ Deployment

### Local Development
```bash
docker-compose up -d        # Start all services
docker-compose logs -f      # View logs
docker-compose down         # Stop all services
```

### GCP Production Deployment
```bash
# 1. Set up GCP infrastructure with Terraform
cd infrastructure/terraform
terraform init
terraform plan
terraform apply

# 2. Build & push Docker images to Artifact Registry
bash scripts/build_and_push.sh

# 3. Deploy to GKE
kubectl apply -f infrastructure/kubernetes/

# 4. Verify deployment
kubectl get pods -n climate-platform
```

### CI/CD Pipeline (GitHub Actions)
```
Push to feature branch â†’ Lint & Test â†’ Build Docker images
                                              â”‚
Merge to main â†’ Build â†’ Push to Artifact Registry â†’ Deploy to GKE
```

---

## ğŸ“Š Monitoring

| Tool | What It Monitors |
|------|-----------------|
| **Prometheus** | Service uptime, API latency, Kafka lag, Spark job duration |
| **Grafana** | Visual dashboards for all metrics with alerting |
| **Evidently AI** | Data drift (input feature distribution changes), model performance drift (accuracy/F1 degradation) |
| **MLflow** | Experiment history, model versions, comparison across runs |
| **Airflow UI** | DAG run status, task failures, retry history |

---

## ğŸ“¡ API Documentation

Full interactive docs available at `http://localhost:8000/docs` (Swagger UI) when running locally.

### Key Endpoints

```
POST   /api/v1/predict          â†’ Get extreme weather prediction for a location
POST   /api/v1/chat             â†’ Ask a climate question (Gen AI chatbot)
GET    /api/v1/anomalies        â†’ List recent anomalies with LLM explanations
GET    /api/v1/reports/{date}   â†’ Get generated weather report for a date
GET    /api/v1/health           â†’ Service health check
```

### Example Request
```bash
curl -X POST http://localhost:8000/api/v1/predict \
  -H "Content-Type: application/json" \
  -d '{
    "latitude": 33.4484,
    "longitude": -112.0740,
    "forecast_hours": 72
  }'
```

### Example Response
```json
{
  "location": "Phoenix, AZ",
  "predictions": [
    {
      "event_type": "heatwave",
      "probability": 0.89,
      "severity": "extreme",
      "confidence_interval": [0.82, 0.94],
      "forecast_window": "2025-07-15 to 2025-07-18",
      "explanation": "Persistent high-pressure ridge combined with record soil moisture deficit indicates extreme heat event."
    }
  ],
  "model_version": "ensemble-v2.3.1",
  "generated_report": "A severe heatwave is expected in Phoenix, AZ..."
}
```

---

## ğŸ“… Implementation Roadmap

| Week | Focus | Key Deliverables |
|------|-------|-----------------|
| **1** | Foundation & Kafka | Project setup, Docker Compose, Kafka cluster running, first message flowing |
| **2** | Data Ingestion | Python producers for NOAA + OpenWeather, Avro schemas, Kafka Connect to GCS |
| **3** | Spark Processing | Structured Streaming (Kafka â†’ Bronze), batch jobs (Silver, Gold), GCS partitioning |
| **4** | BigQuery & Airflow | Star schema design, Gold â†’ BigQuery load, Airflow DAGs, Great Expectations |
| **5** | ML Models & MLflow | XGBoost + LSTM training, ensemble, SHAP, Optuna, MLflow tracking |
| **6** | Gen AI Integration | ChromaDB + RAG, report generator, anomaly explainer, text-to-SQL |
| **7** | API & Dashboard | FastAPI endpoints, Streamlit dashboard, Evidently drift monitoring |
| **8** | Deployment & CI/CD | Dockerize all services, Terraform, GKE deploy, GitHub Actions, Grafana |

---

## ğŸ“ Why This Project Stands Out

- **End-to-end**: From raw API data to Gen AI-powered insights â€” not just a notebook
- **Production-grade**: Kafka, Docker, Kubernetes, CI/CD, monitoring â€” how real systems work
- **Modern stack**: Combines traditional big data (Spark) with cutting-edge Gen AI (RAG, LLMs)
- **Explainable AI**: SHAP values + LLM explanations â€” not a black box
- **Scalable**: Designed to handle terabytes with proper partitioning, streaming, and auto-scaling

---

## ğŸ“„ License

This project is licensed under the MIT License. See [LICENSE](LICENSE) for details.

---

## ğŸ¤ Contributing

Contributions are welcome. Please read [CONTRIBUTING.md](docs/CONTRIBUTING.md) for guidelines.

---

*Built with â¤ï¸ for climate resilience*# ğŸŒ Climate Intelligence Platform for Extreme Weather Prediction

> A **production-grade data engineering & machine learning project** that predicts extreme weather events using real NOAA weather data, Apache Kafka, PostgreSQL, ML models, and Google Cloud Platform.

[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)
[![Docker](https://img.shields.io/badge/docker-required-brightgreen.svg)](https://www.docker.com/)
[![License](https://img.shields.io/badge/license-MIT-green.svg)](LICENSE)
[![Status](https://img.shields.io/badge/status-Active-success.svg)]()

---

## ğŸ“– Table of Contents

1. [Project Overview](#-project-overview)
2. [Features](#-features)
3. [Architecture](#-architecture)
4. [Tech Stack](#-tech-stack)
5. [Quick Start](#-quick-start)
6. [Project Structure](#-project-structure)
7. [Detailed Setup](#-detailed-setup)
8. [Usage Guide](#-usage-guide)
9. [Implementation Phases](#-implementation-phases)
10. [Data Pipeline](#-data-pipeline)
11. [Machine Learning](#-machine-learning)
12. [API Documentation](#-api-documentation)
13. [Deployment](#-deployment)
14. [Monitoring & Logging](#-monitoring--logging)
15. [Troubleshooting](#-troubleshooting)
16. [Contributing](#-contributing)
17. [Learning Resources](#-learning-resources)
18. [FAQ](#-faq)

---

# ğŸ¯ Project Overview

## What This Project Does

A **complete, production-ready data engineering platform** that:

1. **Ingests** real weather data from 15 major US cities via NOAA API
2. **Streams** data through Apache Kafka (no data loss)
3. **Processes** & transforms data in real-time
4. **Stores** in PostgreSQL and Google Cloud BigQuery
5. **Trains** ensemble ML models (LSTM + XGBoost)
6. **Predicts** extreme weather events
7. **Serves** predictions via FastAPI
8. **Visualizes** on interactive Streamlit dashboard
9. **Deploys** to Kubernetes & Google Cloud
10. **Monitors** with Prometheus, Grafana, and Cloud Logging

## Real-World Use Cases

| Use Case | Benefit |
|----------|---------|
| **Insurance Companies** | Assess weather risk for policies |
| **Weather Services** | Improve extreme weather alerts |
| **Agriculture** | Monitor crop conditions |
| **Emergency Management** | Plan disaster response |
| **City Planning** | Understand climate patterns |
| **Energy Sector** | Forecast demand based on weather |

---

# âœ¨ Features

## Core Features (Implemented)

âœ… **Real-time Data Ingestion**
- NOAA API integration (no API key required!)
- 15 major US cities monitored
- Updates every 30 seconds
- Automatic error handling & retries

âœ… **Stream Processing**
- Apache Kafka message queue
- Real-time data validation
- Feature engineering (heat index calculation)
- Extreme event detection
- Data deduplication

âœ… **Data Storage**
- PostgreSQL for transactional data
- Structured schema with migrations
- Indexed queries for performance
- Data retention policies

âœ… **Data Quality**
- Schema validation
- Range validation (temperature, humidity, etc.)
- Anomaly detection
- Error logging & alerting

## Advanced Features (Coming Soon)

ğŸ”œ **Machine Learning**
- LSTM time-series forecasting
- XGBoost classification
- Ensemble predictions
- Model versioning & registry
- Hyperparameter tuning
- Drift detection & automated retraining

ğŸ”œ **REST API**
- FastAPI with auto-documentation
- Multiple prediction endpoints
- Request/response logging
- Rate limiting & authentication
- Redis caching layer
- Health check endpoints

ğŸ”œ **Interactive Dashboard**
- Streamlit web application
- Real-time predictions
- Historical analysis
- Model performance metrics
- Alert management
- Admin console

ğŸ”œ **Cloud Deployment**
- Google Cloud integration
- Pub/Sub messaging
- Dataflow stream processing
- BigQuery data warehouse
- Vertex AI ML training
- Cloud Run serverless hosting

ğŸ”œ **Orchestration**
- Apache Airflow DAGs
- Cloud Composer managed workflows
- Scheduled retraining jobs
- Data quality checks
- SLA monitoring

ğŸ”œ **Kubernetes**
- Docker containerization
- Kubernetes manifests
- Helm charts
- Auto-scaling policies
- Blue-green deployments

ğŸ”œ **CI/CD Pipeline**
- GitHub Actions automation
- Automated testing
- Performance benchmarking
- Container registry
- Automated deployments

---

# ğŸ—ï¸ Architecture

## High-Level Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        DATA SOURCES                              â”‚
â”‚    NOAA Weather API (15 US Cities) | Satellite Data             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              DATA INGESTION LAYER (Real-time)                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ NOAA Client â†’ Kafka Producer â†’ Kafka Topic             â”‚   â”‚
â”‚  â”‚ â€¢ Fetch real weather every 30 seconds                  â”‚   â”‚
â”‚  â”‚ â€¢ Handle API failures gracefully                       â”‚   â”‚
â”‚  â”‚ â€¢ Publish to "weather-events" topic                    â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         STREAM PROCESSING LAYER (Data Transformation)           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ Kafka Consumer â†’ Data Processor â†’ PostgreSQL            â”‚   â”‚
â”‚  â”‚ â€¢ Consume from Kafka topic                              â”‚   â”‚
â”‚  â”‚ â€¢ Validate data ranges                                  â”‚   â”‚
â”‚  â”‚ â€¢ Calculate heat index (new feature)                    â”‚   â”‚
â”‚  â”‚ â€¢ Detect extreme events (new feature)                   â”‚   â”‚
â”‚  â”‚ â€¢ Add processing timestamp                              â”‚   â”‚
â”‚  â”‚ â€¢ Store raw & processed data                            â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            DATA WAREHOUSE LAYER (Storage & Query)               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ PostgreSQL (Local) | BigQuery (Cloud)                   â”‚   â”‚
â”‚  â”‚ â€¢ Store raw events                                      â”‚   â”‚
â”‚  â”‚ â€¢ Store processed data                                  â”‚   â”‚
â”‚  â”‚ â€¢ Historical data for analysis                          â”‚   â”‚
â”‚  â”‚ â€¢ Ready for ML training                                 â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              ML/AI LAYER (Coming Soon)                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ LSTM + XGBoost Ensemble                                 â”‚   â”‚
â”‚  â”‚ â€¢ Time-series forecasting (LSTM)                        â”‚   â”‚
â”‚  â”‚ â€¢ Event classification (XGBoost)                        â”‚   â”‚
â”‚  â”‚ â€¢ Ensemble voting                                       â”‚   â”‚
â”‚  â”‚ â€¢ Model versioning                                      â”‚   â”‚
â”‚  â”‚ â€¢ Drift detection & retraining                          â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            SERVING LAYER (API & Caching)                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ FastAPI Server | Vertex AI Endpoints                    â”‚   â”‚
â”‚  â”‚ â€¢ REST API endpoints                                    â”‚   â”‚
â”‚  â”‚ â€¢ Redis caching                                         â”‚   â”‚
â”‚  â”‚ â€¢ Authentication & rate limiting                        â”‚   â”‚
â”‚  â”‚ â€¢ Response logging                                      â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           FRONTEND LAYER (User Interfaces)                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ Streamlit Dashboard | React SPA (Coming)                â”‚   â”‚
â”‚  â”‚ â€¢ Real-time predictions                                 â”‚   â”‚
â”‚  â”‚ â€¢ Historical analytics                                  â”‚   â”‚
â”‚  â”‚ â€¢ Model metrics & performance                           â”‚   â”‚
â”‚  â”‚ â€¢ Alert management                                      â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      MONITORING & ORCHESTRATION (Observability)                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ Prometheus + Grafana | Airflow | Cloud Logging         â”‚   â”‚
â”‚  â”‚ â€¢ Metrics collection                                    â”‚   â”‚
â”‚  â”‚ â€¢ Pipeline orchestration                                â”‚   â”‚
â”‚  â”‚ â€¢ Automated alerting                                    â”‚   â”‚
â”‚  â”‚ â€¢ Log aggregation                                       â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

# ğŸ’» Tech Stack

## Core Technologies

### **Data Ingestion & Streaming**
- **NOAA Weather API** - Real weather data source
- **Apache Kafka** - Message broker for streaming
- **Python Requests** - HTTP client for APIs

### **Data Processing**
- **Apache Spark** - Distributed processing
- **Python/Pandas** - Data manipulation
- **Great Expectations** - Data validation

### **Storage**
- **PostgreSQL** - Primary transactional database
- **Google Cloud BigQuery** - Cloud data warehouse
- **Cloud Storage** - Object storage

### **Machine Learning**
- **TensorFlow/Keras** - LSTM neural networks
- **XGBoost** - Gradient boosting
- **Scikit-learn** - ML utilities
- **PyTorch** - Deep learning (optional)

### **API & Web**
- **FastAPI** - REST API framework
- **Streamlit** - Dashboard framework
- **Redis** - Caching layer
- **Pydantic** - Data validation

### **Cloud Platform**
- **Google Cloud Platform (GCP)**
  - Pub/Sub
  - Cloud Dataflow
  - BigQuery
  - Vertex AI
  - Cloud Run
  - Cloud Storage

### **Orchestration**
- **Apache Airflow** - Workflow orchestration
- **Cloud Composer** - Managed Airflow on GCP

### **Monitoring & Logging**
- **Prometheus** - Metrics collection
- **Grafana** - Visualization
- **Cloud Logging** - Log aggregation
- **Cloud Monitoring** - GCP monitoring

### **DevOps & Deployment**
- **Docker** - Container runtime
- **Docker Compose** - Local orchestration
- **Kubernetes** - Container orchestration
- **Helm** - K8s package manager
- **Terraform** - Infrastructure as Code
- **GitHub Actions** - CI/CD pipeline

### **Development Tools**
- **Python 3.10+**
- **Git** - Version control
- **VS Code** - IDE
- **Jupyter** - Notebooks

---

# ğŸš€ Quick Start

## Prerequisites

```bash
# Required
- Windows, macOS, or Linux
- Python 3.10 or higher
- Docker & Docker Compose
- Git
- 8GB+ RAM
- 20GB+ disk space
```

## 5-Minute Setup

```bash
# 1. Clone or download the project
cd Climate-Intelligence-Platform

# 2. Create virtual environment
python -m venv venv
.\venv\Scripts\Activate.ps1  # Windows
source venv/bin/activate     # macOS/Linux

# 3. Install dependencies
pip install -r requirements.txt

# 4. Start Docker services
docker-compose up -d

# 5. Create database tables
python data_processing\init_database.py

# 6. Start the pipeline (3 terminals)
# Terminal 1:
python data_ingestion\real_kafka_producer.py

# Terminal 2:
python data_processing\data_processor.py

# Terminal 3:
python data_processing\view_data.py
```

**That's it! Real weather data is now flowing through your pipeline!** ğŸ‰

---

# ğŸ“ Project Structure

```
climate-intelligence-platform/
â”‚
â”œâ”€â”€ ğŸ“ data_ingestion/                    # Get real weather data
â”‚   â”œâ”€â”€ noaa_weather_client.py            # NOAA API integration
â”‚   â”œâ”€â”€ real_kafka_producer.py            # Send to Kafka
â”‚   â”œâ”€â”€ kafka_consumer.py                 # Test receiver
â”‚   â””â”€â”€ requirements.txt
â”‚
â”œâ”€â”€ ğŸ“ data_processing/                   # Clean & transform data
â”‚   â”œâ”€â”€ init_database.py                  # Create DB tables
â”‚   â”œâ”€â”€ data_processor.py                 # Main processor
â”‚   â”œâ”€â”€ view_data.py                      # Display results
â”‚   â”œâ”€â”€ transformations.py                # Feature engineering
â”‚   â””â”€â”€ data_quality.py                   # Validation
â”‚
â”œâ”€â”€ ğŸ“ ml_training/                       # Machine Learning
â”‚   â”œâ”€â”€ data_prep.py                      # Prepare training data
â”‚   â”œâ”€â”€ feature_store.py                  # Feature management
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ lstm_model.py                 # LSTM architecture
â”‚   â”‚   â”œâ”€â”€ xgboost_model.py              # XGBoost classifier
â”‚   â”‚   â””â”€â”€ ensemble.py                   # Ensemble approach
â”‚   â”œâ”€â”€ train.py                          # Training script
â”‚   â”œâ”€â”€ evaluate.py                       # Model evaluation
â”‚   â”œâ”€â”€ hyperparameter_tuning.py          # Optimize hyperparameters
â”‚   â””â”€â”€ requirements.txt
â”‚
â”œâ”€â”€ ğŸ“ model_serving/                     # Serve predictions
â”‚   â”œâ”€â”€ main.py                           # FastAPI app
â”‚   â”œâ”€â”€ routes/
â”‚   â”‚   â”œâ”€â”€ predict.py                    # Prediction endpoint
â”‚   â”‚   â”œâ”€â”€ health.py                     # Health checks
â”‚   â”‚   â””â”€â”€ metrics.py                    # Performance metrics
â”‚   â”œâ”€â”€ schemas.py                        # Data validation
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ requirements.txt
â”‚
â”œâ”€â”€ ğŸ“ dashboard/                         # User interface
â”‚   â”œâ”€â”€ streamlit_app.py                  # Main app
â”‚   â”œâ”€â”€ pages/
â”‚   â”‚   â”œâ”€â”€ predictions.py                # Prediction page
â”‚   â”‚   â”œâ”€â”€ historical_analysis.py        # Analytics
â”‚   â”‚   â”œâ”€â”€ model_metrics.py              # Model performance
â”‚   â”‚   â””â”€â”€ alerts.py                     # Alert management
â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â”œâ”€â”€ charts.py                     # Visualizations
â”‚   â”‚   â””â”€â”€ utils.py                      # Helper functions
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ requirements.txt
â”‚
â”œâ”€â”€ ğŸ“ orchestration/                     # Workflow scheduling
â”‚   â”œâ”€â”€ airflow/
â”‚   â”‚   â”œâ”€â”€ airflow_dag.py                # Local Airflow
â”‚   â”‚   â”œâ”€â”€ dags/
â”‚   â”‚   â”‚   â”œâ”€â”€ data_pipeline_dag.py      # Data pipeline
â”‚   â”‚   â”‚   â”œâ”€â”€ ml_training_dag.py        # Training jobs
â”‚   â”‚   â”‚   â””â”€â”€ monitoring_dag.py         # Monitoring
â”‚   â”‚   â””â”€â”€ plugins/
â”‚   â”œâ”€â”€ cloud_composer/
â”‚   â”‚   â””â”€â”€ composer_dag.py               # GCP Cloud Composer
â”‚   â””â”€â”€ jobs/
â”‚       â”œâ”€â”€ daily_retraining.py
â”‚       â”œâ”€â”€ data_validation.py
â”‚       â””â”€â”€ model_monitoring.py
â”‚
â”œâ”€â”€ ğŸ“ monitoring/                        # Observability
â”‚   â”œâ”€â”€ prometheus/
â”‚   â”‚   â”œâ”€â”€ prometheus.yml                # Config
â”‚   â”‚   â””â”€â”€ alerts.yml                    # Alerting rules
â”‚   â”œâ”€â”€ grafana/
â”‚   â”‚   â”œâ”€â”€ dashboards/                   # Grafana dashboards
â”‚   â”‚   â””â”€â”€ datasources/                  # Data sources
â”‚   â””â”€â”€ logging/
â”‚       â”œâ”€â”€ logging_config.py
â”‚       â””â”€â”€ structured_logging.py
â”‚
â”œâ”€â”€ ğŸ“ infrastructure/                    # DevOps & Deployment
â”‚   â”œâ”€â”€ docker/
â”‚   â”‚   â”œâ”€â”€ Dockerfile.ingestion
â”‚   â”‚   â”œâ”€â”€ Dockerfile.processing
â”‚   â”‚   â”œâ”€â”€ Dockerfile.api
â”‚   â”‚   â””â”€â”€ Dockerfile.dashboard
â”‚   â”œâ”€â”€ kubernetes/
â”‚   â”‚   â”œâ”€â”€ deployment.yaml
â”‚   â”‚   â”œâ”€â”€ service.yaml
â”‚   â”‚   â”œâ”€â”€ configmap.yaml
â”‚   â”‚   â””â”€â”€ helm-chart/
â”‚   â”œâ”€â”€ terraform/
â”‚   â”‚   â”œâ”€â”€ main.tf
â”‚   â”‚   â”œâ”€â”€ variables.tf
â”‚   â”‚   â”œâ”€â”€ cloud_run.tf
â”‚   â”‚   â”œâ”€â”€ bigquery.tf
â”‚   â”‚   â””â”€â”€ pubsub.tf
â”‚   â””â”€â”€ scripts/
â”‚       â”œâ”€â”€ setup_gcp.sh
â”‚       â”œâ”€â”€ deploy_docker.sh
â”‚       â”œâ”€â”€ deploy_k8s.sh
â”‚       â””â”€â”€ cleanup.sh
â”‚
â”œâ”€â”€ ğŸ“ tests/                             # Testing
â”‚   â”œâ”€â”€ unit/
â”‚   â”‚   â”œâ”€â”€ test_ingestion.py
â”‚   â”‚   â”œâ”€â”€ test_processing.py
â”‚   â”‚   â”œâ”€â”€ test_models.py
â”‚   â”‚   â””â”€â”€ test_api.py
â”‚   â”œâ”€â”€ integration/
â”‚   â”‚   â”œâ”€â”€ test_pipeline.py
â”‚   â”‚   â””â”€â”€ test_ml_pipeline.py
â”‚   â””â”€â”€ e2e/
â”‚       â””â”€â”€ test_full_flow.py
â”‚
â”œâ”€â”€ ğŸ“ notebooks/                         # Jupyter Notebooks
â”‚   â”œâ”€â”€ 01_exploratory_analysis.ipynb
â”‚   â”œâ”€â”€ 02_feature_engineering.ipynb
â”‚   â”œâ”€â”€ 03_model_development.ipynb
â”‚   â””â”€â”€ 04_model_comparison.ipynb
â”‚
â”œâ”€â”€ ğŸ“ .github/                           # GitHub configuration
â”‚   â””â”€â”€ workflows/
â”‚       â”œâ”€â”€ test.yml                      # CI tests
â”‚       â”œâ”€â”€ build.yml                     # Build images
â”‚       â””â”€â”€ deploy.yml                    # Deployment
â”‚
â”œâ”€â”€ docker-compose.yml                    # Local development
â”œâ”€â”€ docker-compose.prod.yml               # Production stack
â”œâ”€â”€ requirements.txt                      # Python dependencies
â”œâ”€â”€ requirements-dev.txt                  # Development dependencies
â”œâ”€â”€ .env.example                          # Environment template
â”œâ”€â”€ .gitignore
â”œâ”€â”€ Dockerfile                            # Main image
â”œâ”€â”€ README.md                             # This file!
â”œâ”€â”€ ARCHITECTURE.md                       # Detailed architecture
â”œâ”€â”€ DEPLOYMENT.md                         # Deployment guide
â”œâ”€â”€ CONTRIBUTING.md                       # Contributing guide
â””â”€â”€ LICENSE
```

---

# ğŸ”§ Detailed Setup

## Step 1: Clone the Repository

```bash
git clone https://github.com/your-username/climate-intelligence.git
cd climate-intelligence
```

## Step 2: Create Virtual Environment

```bash
# Windows
python -m venv venv
.\venv\Scripts\Activate.ps1

# macOS/Linux
python3 -m venv venv
source venv/bin/activate
```

## Step 3: Install Dependencies

```bash
pip install --upgrade pip
pip install -r requirements.txt
```

## Step 4: Configure Environment

```bash
# Copy example environment file
cp .env.example .env

# Edit .env with your settings
# nano .env  or  code .env
```

## Step 5: Start Docker Services

```bash
# Start all services
docker-compose up -d

# Verify services are running
docker-compose ps

# Check logs
docker-compose logs -f
```

## Step 6: Initialize Database

```bash
python data_processing\init_database.py
```

You should see:
```
âœ… Connected to PostgreSQL
âœ… Tables created successfully
```

## Step 7: Run the Pipeline

Open **3 separate terminals**:

**Terminal 1 - Producer:**
```bash
python data_ingestion\real_kafka_producer.py
```

**Terminal 2 - Processor:**
```bash
python data_processing\data_processor.py
```

**Terminal 3 - Viewer:**
```bash
python data_processing\view_data.py
```

---

# ğŸ“Š Usage Guide

## Running Each Component

### Producer (Real Weather Data)
```bash
python data_ingestion\real_kafka_producer.py
```
- Fetches REAL weather from NOAA every 30 seconds
- Sends 15 cities worth of data per fetch
- Runs for 5 minutes (300 seconds)
- Press Ctrl+C to stop

### Processor (Transform & Store)
```bash
python data_processing\data_processor.py
```
- Reads data from Kafka
- Validates and transforms
- Stores in PostgreSQL
- Runs indefinitely (press Ctrl+C to stop)

### View Data (Display Results)
```bash
python data_processing\view_data.py
```
- Queries last 20 records from database
- Displays in formatted table
- Shows all transformed features

## Docker Commands

```bash
# Start all services
docker-compose up -d

# Stop all services
docker-compose down

# View logs
docker-compose logs -f [service-name]

# Stop specific service
docker-compose stop kafka

# Restart a service
docker-compose restart postgres

# View running services
docker-compose ps

# Remove all data (WARNING!)
docker-compose down -v
```

## Database Queries

```bash
# Connect to PostgreSQL
psql -h localhost -U airflow -d airflow

# View tables
\dt

# Query raw events
SELECT * FROM raw_events LIMIT 10;

# Query processed data
SELECT * FROM processed_data LIMIT 10;

# Count records
SELECT COUNT(*) FROM processed_data;

# Find extreme events
SELECT * FROM processed_data WHERE extreme_event = 1;

# Exit
\q
```

---

# ğŸ“ˆ Implementation Phases

## Phase 1: Local Development âœ… COMPLETED
- [x] Docker setup with Kafka, PostgreSQL, Redis
- [x] Python environment & dependencies
- [x] Project structure
- [x] Basic documentation

## Phase 2: Data Ingestion âœ… COMPLETED
- [x] NOAA API integration
- [x] Kafka producer
- [x] Kafka consumer (test)
- [x] Real weather data streaming
- [x] Error handling & retries

## Phase 3: Data Processing âœ… COMPLETED
- [x] Data validation
- [x] Feature engineering (heat index)
- [x] Extreme event detection
- [x] Database storage
- [x] Data quality checks

## Phase 4: Machine Learning ğŸ”œ IN PROGRESS
- [ ] LSTM model development
- [ ] XGBoost model
- [ ] Ensemble approach
- [ ] Hyperparameter tuning
- [ ] Model evaluation & comparison
- [ ] Cross-validation

## Phase 5: API & Dashboard ğŸ”œ COMING SOON
- [ ] FastAPI server
- [ ] REST endpoints
- [ ] Authentication
- [ ] Streamlit dashboard
- [ ] Real-time predictions
- [ ] Historical analytics

## Phase 6: GCP Cloud Deployment ğŸ”œ COMING SOON
- [ ] GCP project setup
- [ ] Pub/Sub configuration
- [ ] Cloud Dataflow pipelines
- [ ] BigQuery datasets
- [ ] Vertex AI training
- [ ] Cloud Run deployment

## Phase 7: Kubernetes & CI/CD ğŸ”œ COMING SOON
- [ ] Docker images
- [ ] Kubernetes manifests
- [ ] Helm charts
- [ ] GitHub Actions
- [ ] Automated testing
- [ ] Blue-green deployments

## Phase 8: Monitoring & Production ğŸ”œ COMING SOON
- [ ] Prometheus metrics
- [ ] Grafana dashboards
- [ ] Cloud Logging
- [ ] Alerting
- [ ] SLA monitoring
- [ ] Cost optimization

---

# ğŸ”„ Data Pipeline

## Data Transformation Example

### Input (Raw NOAA Data)
```json
{
  "timestamp": "2024-01-20T10:30:45.123456",
  "location": "New York",
  "temperature": 45.0,
  "humidity": 65.0,
  "wind_speed": 12.5,
  "pressure": 1013.25,
  "precipitation": 0.0,
  "forecast_text": "Partly cloudy",
  "is_daytime": true
}
```

### Processing Steps

1. **Validation**
   - âœ… Temperature: -50 to 150Â°F
   - âœ… Humidity: 0-100%
   - âœ… Wind speed: â‰¥0 mph
   - âœ… All fields present

2. **Feature Engineering**
   - Calculate heat index (45.0Â°F - no adjustment needed below 80Â°F)
   - Detect extreme events (45Â°F < 95Â°F, 12.5 < 50 mph â†’ Normal)

3. **Enrichment**
   - Add processed_at timestamp
   - Add data source metadata
   - Add processing version

### Output (Processed Data)
```json
{
  "timestamp": "2024-01-20T10:30:45.123456",
  "location": "New York",
  "temperature": 45.0,
  "humidity": 65.0,
  "wind_speed": 12.5,
  "pressure": 1013.25,
  "precipitation": 0.0,
  "heat_index": 45.0,
  "extreme_event": 0,
  "processed_at": "2024-01-20T10:35:20.654321"
}
```

### Stored in Database
Both raw and processed data stored in PostgreSQL:
- `raw_events` table - Original data
- `processed_data` table - Transformed data

### Displayed in Terminal
```
Location         Temp    Humidity  Wind    Heat Idx  Extreme
New York         45.0    65.0      12.5    45.0      0
Los Angeles      72.5    45.0      8.3     72.5      0
Chicago          38.1    55.0      15.2    38.1      0
Houston          68.5    70.0      10.5    69.2      0
Phoenix          85.2    30.0      12.3    85.2      0
```

---

# ğŸ§  Machine Learning

## Models (Coming Soon)

### LSTM (Long Short-Term Memory)
- **Purpose:** Time-series forecasting
- **Input:** 24 hours of weather data
- **Output:** Next hour temperature prediction
- **Architecture:** 128 â†’ 64 â†’ 32 â†’ 1

### XGBoost
- **Purpose:** Extreme event classification
- **Input:** Current weather features
- **Output:** Probability of extreme event (0-1)
- **Tree Depth:** 8, Learning Rate: 0.1

### Ensemble
- **Approach:** Weighted voting
- **LSTM Weight:** 40%
- **XGBoost Weight:** 60%
- **Final Prediction:** Ensemble confidence

## Training Pipeline
```
Historical Data (90 days)
    â†“
Feature Engineering
    â†“
Train/Validation Split (80/20)
    â†“
Model Training
    â†“
Hyperparameter Tuning
    â†“
Cross-Validation
    â†“
Model Evaluation
    â†“
Registry & Versioning
    â†“
Deployment
```

---

# ğŸŒ API Documentation

## REST Endpoints (Coming Soon)

### Predict Endpoint
```bash
POST /api/v1/predict

Request:
{
  "temperature": 45.0,
  "humidity": 65.0,
  "wind_speed": 12.5,
  "pressure": 1013.25,
  "location": "New York"
}

Response:
{
  "prediction": "Normal",
  "confidence": 0.95,
  "lstm_confidence": 0.92,
  "xgboost_confidence": 0.97,
  "timestamp": "2024-01-20T10:35:20"
}
```

### Health Check
```bash
GET /api/v1/health

Response:
{
  "status": "healthy",
  "models_loaded": true,
  "database_connected": true,
  "cache_available": true
}
```

### Metrics Endpoint
```bash
GET /api/v1/metrics

Response:
{
  "total_predictions": 1000,
  "extreme_events_detected": 15,
  "average_confidence": 0.93,
  "avg_response_time_ms": 45
}
```

---

# ğŸš€ Deployment

## Local Deployment (Current)

```bash
docker-compose up -d
```

## Docker Deployment (Coming)

```bash
docker build -t climate-intelligence:latest .
docker run -p 8000:8000 climate-intelligence:latest
```

## Kubernetes Deployment (Coming)

```bash
kubectl apply -f kubernetes/
helm install climate-intelligence ./helm-chart
```

## GCP Cloud Deployment (Coming)

```bash
# Deploy to Cloud Run
gcloud run deploy climate-api \
  --image gcr.io/PROJECT/climate-intelligence \
  --memory 2Gi \
  --region us-central1
```

---

# ğŸ“Š Monitoring & Logging

## Local Monitoring

```bash
# Prometheus metrics
http://localhost:9090

# Grafana dashboard
http://localhost:3000

# View logs
docker-compose logs -f [service]
```

## Metrics Collected
- Pipeline latency (ms)
- Data quality score
- Model prediction confidence
- Database query time
- API response time
- Data throughput (records/sec)
- Error rates

## Alerts
- High latency (>500ms)
- Low data quality (<95%)
- Model drift detected
- Database connection failed
- API errors (>5% failure rate)

---

# ğŸ› Troubleshooting

## Common Issues & Solutions

### "relation 'raw_events' does not exist"
**Cause:** Database tables not created
**Solution:**
```bash
python data_processing\init_database.py
```

### "No module named 'psycopg2'"
**Cause:** Missing PostgreSQL adapter
**Solution:**
```bash
pip install psycopg2-binary
```

### "NoBrokersAvailable"
**Cause:** Kafka not running
**Solution:**
```bash
docker-compose up -d
docker-compose ps  # Verify all services are Up
```

### "Connection refused"
**Cause:** Docker services not healthy
**Solution:**
```bash
docker-compose down
docker-compose up -d
docker-compose ps  # Wait for (healthy) status
```

### "ModuleNotFoundError: No module named 'data_ingestion'"
**Cause:** Running from wrong directory
**Solution:**
```bash
cd project-root-directory
python data_ingestion\real_kafka_producer.py
```

### Database connection timeout
**Cause:** PostgreSQL starting slowly
**Solution:**
```bash
# Wait 30 seconds, then try again
sleep 30
python data_processing\data_processor.py
```

### Kafka topics not created
**Cause:** Auto-creation disabled
**Solution:**
```bash
docker exec kafka kafka-topics --bootstrap-server localhost:9092 \
  --create --topic weather-events --partitions 1 --replication-factor 1
```

## Debug Mode

```bash
# Enable debug logging
export LOG_LEVEL=DEBUG

# Run with verbose output
python -u data_processing\data_processor.py

# Check all Docker services
docker-compose logs --tail=100

# Test Kafka connectivity
docker exec kafka kafka-broker-api-versions --bootstrap-server localhost:9092
```

---

# ğŸ¤ Contributing

We welcome contributions! Here's how:

## Setup Development Environment

```bash
pip install -r requirements-dev.txt
pre-commit install
```

## Development Workflow

1. Create feature branch: `git checkout -b feature/my-feature`
2. Make changes and commit: `git commit -m "Add feature"`
3. Push: `git push origin feature/my-feature`
4. Create Pull Request with description

## Code Standards

- Python: PEP 8
- Docstrings: Google style
- Type hints: Required for new code
- Tests: 80%+ coverage

## Testing

```bash
# Run all tests
pytest

# Run with coverage
pytest --cov=src

# Run specific test
pytest tests/unit/test_ingestion.py

# Run integration tests
pytest tests/integration/
```

---

# ğŸ“š Learning Resources

## Official Documentation
- [NOAA Weather API](https://www.weather.gov/documentation/services-web-api)
- [Apache Kafka](https://kafka.apache.org/documentation/)
- [PostgreSQL](https://www.postgresql.org/docs/)
- [TensorFlow/Keras](https://www.tensorflow.org/guide)
- [XGBoost](https://xgboost.readthedocs.io/)
- [FastAPI](https://fastapi.tiangolo.com/)
- [Streamlit](https://docs.streamlit.io/)
- [Google Cloud](https://cloud.google.com/docs)

## Tutorials & Courses
- [Apache Kafka in 30 Minutes](https://www.youtube.com/watch?v=06iRM1Ghr1k)
- [PostgreSQL Tutorial](https://www.postgresqltutorial.com/)
- [Deep Learning Time Series](https://www.deeplearningbook.org/)
- [FastAPI Full Course](https://www.youtube.com/watch?v=7t2alSnE2-I)
- [Docker Mastery](https://www.udemy.com/course/docker-mastery/)
- [Kubernetes in 100 Seconds](https://www.youtube.com/watch?v=cC46cg5FFAM)

## Books
- "Designing Machine Learning Systems" by Chip Huyen
- "Fundamentals of Software Architecture" by Mark Richards
- "Site Reliability Engineering" by Google
- "The Data Warehouse Toolkit" by Ralph Kimball

---

# â“ FAQ

**Q: Do I need API keys?**
A: No! NOAA API is completely free with no authentication.

**Q: Can I use this on macOS/Linux?**
A: Yes! All code is cross-platform.

**Q: What's the minimum hardware required?**
A: 4GB RAM, 20GB disk (8GB RAM recommended).

**Q: How much does this cost?**
A: Completely free locally. GCP has free tier but charges after.

**Q: How often is data updated?**
A: Every 30 seconds from NOAA.

**Q: How many historical days of data do I need?**
A: ML training starts with 90 days recommended.

**Q: Can I add more cities?**
A: Yes! Edit noaa_weather_client.py and add coordinates.

**Q: What's the data retention policy?**
A: No automatic deletion. Keep as long as needed.

**Q: Can I deploy to AWS instead of GCP?**
A: Yes! Architecture is cloud-agnostic.

**Q: Is this production-ready?**
A: Phases 1-3 are. Phases 4-8 need completion.

---

# ğŸ“ Support & Contact

**Issues & Bugs:** Open GitHub Issues
**Questions:** Check FAQ or Discussions
**Contributions:** See CONTRIBUTING.md
**Email:** your-email@example.com

---

# ğŸ“„ License

This project is licensed under the MIT License - see LICENSE file for details.

---

# ğŸ™ Acknowledgments

- NOAA for providing free weather data
- Apache Kafka & community
- PostgreSQL community
- Google Cloud for documentation
- All open-source contributors

---

## ğŸ¯ Key Metrics

- **Data Sources:** 15 US cities
- **Update Frequency:** Every 30 seconds
- **Daily Data Points:** ~43,200 records
- **Database Size:** ~1 MB/month
- **Processing Latency:** <100ms/record
- **Data Accuracy:** 100% (from NOAA)
- **Uptime Target:** 99.9%

---

## ğŸ“Š Project Status

| Phase | Status | Completion | Next |
|-------|--------|-----------|------|
| 1. Setup | âœ… Complete | 100% | Start Phase 2 |
| 2. Ingestion | âœ… Complete | 100% | Start Phase 3 |
| 3. Processing | âœ… Complete | 100% | Start Phase 4 |
| 4. ML | ğŸ”œ In Progress | 0% | Model training |
| 5. API/Dashboard | â³ Planned | 0% | After Phase 4 |
| 6. GCP Deploy | â³ Planned | 0% | After Phase 5 |
| 7. Kubernetes | â³ Planned | 0% | After Phase 6 |
| 8. Monitoring | â³ Planned | 0% | After Phase 7 |

---

**Last Updated:** January 2024
**Maintainer:** Your Name
**Repository:** github.com/your-username/climate-intelligence
